{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Actor-Critic algortihms (BAC)\n",
    "---\n",
    "In this notebook, we train a Bayesian Actor-Critic with DeepMind Control Suite's `Cartpole` domain in `balance` task.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import suite\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "from packaging import version\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bayesian_ddpg import Agent\n",
    "from cpprb import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from utils.logx import EpochLogger\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)\n",
    "STATE_DIM = (5,)\n",
    "ACTION_DIM = 1\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "env = suite.load(domain_name='cartpole', \n",
    "                 task_name='balance')\n",
    "\n",
    "agent = Agent(state_dim=STATE_DIM, \n",
    "              action_dim=ACTION_DIM, \n",
    "              dropout_on_v=0)\n",
    "\n",
    "logger_kwargs=dict()\n",
    "logger = EpochLogger(**logger_kwargs)\n",
    "\n",
    "print('Running on ', agent.device)\n",
    "\n",
    "rb = ReplayBuffer(BUFFER_SIZE, {\"obs\": {\"shape\": (STATE_DIM,)},\n",
    "                                \"act\": {\"shape\": ACTION_DIM},\n",
    "                                \"rew\": {},\n",
    "                                \"next_obs\": {\"shape\": (STATE_DIM,)},\n",
    "                                \"done\": {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=1000; max_t=1e3; print_every=5\n",
    "scores_deque = deque(maxlen=print_every)\n",
    "\n",
    "prevScore = 0\n",
    "for i_episode in trange(1, int(n_episodes)+1):\n",
    "    \n",
    "    time_step = env.reset()\n",
    "    state = np.concatenate( [ time_step.observation[key] \n",
    "                             for key in list( time_step.observation.keys() ) ] )\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(int(max_t)):      \n",
    "        action = agent.get_action(state)\n",
    "        time_step = env.step(action)\n",
    "        reward, done = time_step.reward, time_step.last()\n",
    "        next_state = np.concatenate( [ time_step.observation[key] \n",
    "                                      for key in list( time_step.observation.keys() ) ] )\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if rb.get_stored_size() > BATCH_SIZE:\n",
    "            data = rb.sample(BATCH_SIZE)                \n",
    "            states = data['obs']; actions = data['act']; rewards = data['rew']\n",
    "            next_states = data['next_obs']; dones = data['done']\n",
    "            \n",
    "            agent.train(states, actions, next_states, rewards, dones)\n",
    "        \n",
    "        # Save experience / reward\n",
    "        else:       \n",
    "            rb.add(obs=state, \n",
    "                   act=action, \n",
    "                   next_obs=next_state, \n",
    "                   rew=reward,\n",
    "                   done=done)\n",
    "            \n",
    "        state = next_state\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores_deque.append(score)\n",
    "    \n",
    "    if i_episode % print_every == 0:\n",
    "        \n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Episode', i_episode)\n",
    "        logger.log_tabular('EpScore', score)\n",
    "        logger.log_tabular('PrevScore', prevScore)\n",
    "        logger.log_tabular('EpLen (current)', t)\n",
    "        \n",
    "        # Save models\n",
    "        paths = logger.tf_simple_save(agent)\n",
    "                \n",
    "        prevScore = score\n",
    "        clear_output(True)\n",
    "        logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "# reset frames folder\n",
    "subprocess.call([ 'rm', '-rf', 'frames'])\n",
    "subprocess.call([ 'mkdir', '-p', 'frames'])\n",
    "\n",
    "time_step = env.reset()\n",
    "state = np.concatenate([time_step.observation[key] for key in list(time_step.observation.keys())])\n",
    "\n",
    "agent.actor_local.eval()\n",
    "agent.critic_local.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in trange(0, 700):\n",
    "        action = agent.act(state)\n",
    "        time_step = env.step(action)\n",
    "\n",
    "        image_data = env.physics.render(height=480, width=480, camera_id=0)\n",
    "        img = Image.fromarray(image_data, 'RGB')\n",
    "        img.save(\"frames/frame-%.10d.png\" % t)\n",
    "\n",
    "        state = np.concatenate([time_step.observation[key] for key in list(time_step.observation.keys())])\n",
    "        clear_output(True)\n",
    "        if time_step.last():\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert frames to video\n",
    "img_array = []\n",
    "for filename in sorted(glob.glob('frames/*.png')):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "\n",
    "out = cv2.VideoWriter('project.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
